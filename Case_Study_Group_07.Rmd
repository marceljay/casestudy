---
title: "Case Study Documentatation"
author: "IDA Group 7"
date: "July 2019"
output:
  html_document:
    df_print: paged
---

### Introduction

This document provides all major steps and important information regarding the execution of the Case Study for Group 7 of the IDA Course. The output will be a complete data frame with essential data needed for the Shiny App.

The objective is to identify vehicle types that take a lot of time between the Start of production of parts to the finished vehicle for the duration of Jan-1-2015 to Dez-31-2016. For this, the production times of parts, and vehicles need to be considered and visualized appropriately. Since the subsequent analysis will be used to initiate process improvements for critical vehicle types, the most critical vehicle type needs to be selected in the end.

For this Case Study the following libraries were used.

```{r setup, include=FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(data.table)
library(lubridate) 
library(stringr)

```

### 1. Approach

The decision of most critical vehicle type will be based on the vehicle type with the highest amount and occurrences of "production duration".
In order to create informative box plot diagrams visualizing this attribute, a tidied data frame with relevant data needs to be created from the given raw data. Thus all files containing raw data need to be examined to choose a selection of relevant files for import. For the given objective, the data in files containing attributes related to the production date of parts (Einzelteile), components (Komponente) and vehicles (Fahrzeuge) are necessary to produce the attribute "production duration".

While skimming through the files in the directories "Einzelteil", "Komponente" and "Fahrzeug" using external text editors, MS excel and in command line with head() and tail(), it became clear that the often sequentially named .csv and .txt files within contain columns with date-like values such as "origin", while files in the component and vehicle directories containing "Bestandteile_" in their names serve as keys to match columns in parts, components and vehicles together.

After the import, the resulting data frames are tidied according to the principles of tidy data depending on the given situation. Most often, the values are scanned for NA entries and duplicates, irrelevant columns are removed and a filter is set for the required time period. For the "Bestandteile" files, gather() needs to be applied to multiple column occurrences for parts or components. In select cases, further steps are involved to prepare data frames for transformation.

During transformation, all part, component and vehicle data frames are linked together to create one data frame for each segment via bind_rows(). With the "Bestandteile_" data frames serving as keys, the columns of the three data frames containing the production dates of each segments are merged into one final data frame. While the production date of components are not essential for the objective, the component data frame still serves its purpose as it provides the only means to connect parts to vehicles.

The final data frame contains observations for indivitual vehicles within their respective types with the attributes of the vehicle's production date and the production dates of all parts built into that vehicle. By selecting the highest difference in production date between a vehicle and the built in parts, the production time for that vehicle can be determined and visualized for the Shiny App.

The Shiny App then provides input methods to display box plot diagrams of the vehicle production times based on vehicle types and OEM factories. 



### 2. Import & Tidy of all segments

Since there are 79 files to be considered for our analysis, many being untidy, we split up the importing and tidying to separate modules, that were sourced for joining and data analysis. 

#### Parts

The 38 part files in the `Einzelteile` directory had well over ten different patterns that needed to be identified for a correct import and tidying. Some files had more columns than others, some had differerent date formats, missing line breaks or varios delimiters. 
  
In this chunk, all the files from the specific `Einzelteile` directory are written into a vector:
```{r}
partFileNames <- list.files("Data/Einzelteil")
pathVector <- paste("Data/Einzelteil/", partFileNames, sep="")
```  
  
The .csv files were simplier to import: there were two delimiters and and a few different patterns regarding the amount of variables (columns) and the format of the dates. 

At first, after inspecting the files manually with shell commands, four lists to classify the .csv files depending on their number were created:


```{r}
# CSV with pattern A (clean)
list_A <- c("T04", "T10", "T13", "T14", "T18", "T21", "T26", "T40") # semicolon
list_A2 <- c("T06", "T08", "T19", "T25", "T33", "T37") # comma

# CSV with pattern B (extra cols)
list_B <- c("T12", "T15", "T17", "T23", "T32") # semicolon
list_B2 <- c("T05","T30", "T38") # comma
```

Another function, that helps applying the correct import and tidying function was created. Here .csv files are assigned to the functions via the formerly created lists. Since the .txt files needed various import functions, another helper function `importTXT()` is called, that then determines the final function to import and tidy the file.

```{r}
# Function determines, based on predefined lists, which tidy function shall be applied
determineTidyFunction <- function(filePath) {
  
  if (length(which(str_detect(filePath, list_A))) == 1){
    print(paste("found match in list_A:", filePath)) # console logging
    tidyCSV_a(filePath)
  } else if (length(which(str_detect(filePath, list_A2))) == 1){
    print(paste("found match in list_A2:", filePath)) # console logging
    tidyCSV_a(filePath, ",")
  } else if (length(which(str_detect(filePath, list_B))) == 1){
    print(paste("found match in list_B:", filePath)) # console logging
    tidyCSV_b(filePath)
  } else if (length(which(str_detect(filePath, list_B2))) == 1){
    print(paste("found match in list_B2:", filePath)) # console logging
    tidyCSV_b(filePath, ",")
  }  else {
    print(paste("found TXT file:", filePath)) # console logging
    importTXT(filePath)
  }
}
```

The function `tidyCSV_a` which gets called by`determineTidyFunction()` imports the .csv with format A and A2
and returns a data frame. here varios helper functions are called, but are elaborated later one...

```{r}
tidyCSV_a <- function(path, delim = ";") {
  # Console log
  print(paste0("tidyCSV_a called with path: ", path))
  
  #Import CSV depending on delimiter
  if (delim == ",") {
    df <- read_csv(path)
  } else {
    df <- read_csv2(path)
  }
  
  # Tidy dates in short table
  df <- tidyDate(df)
  
  # Drop columns (prod_date was appended as 11th column)
  df <- df[c(3, 11)]
  
  # Renaming cols
  names(df)[1] <- "global_id"

  # Check for NA values
  importAnalysis(df)
  
  return(df)
}
```

The function `tidyCSV_b` which gets called by`determineTidyFunction()` imports the .csv with format B and B2
and returns a data frame. here varios helper functions are called too. It is different from the previous function in terms of not needing to reformat the date and different columns that can be dropped.

```{r}
tidyCSV_b <- function(path, delim = ";") {
  print("---- called tidyCSV_b ----")
  
  #Import CSV depending on delimiter
  if (delim == ",") {
    df <- read.csv(path, stringsAsFactors = FALSE)
  } else {
    df <- read.csv2(path, stringsAsFactors = FALSE)
  }
  
  # Unite, rename, filter dates
  df <- tidyLong(df)
  
  # Drop columns except the 4 necessary ones
  df <- df[3:4]
  
  # Check for NA values
  importAnalysis(df)
  
  return(df)
}
```

The .txt files in the Parts and Components files were more challenging, requiring a deeper pattern analysis, specially because of the lack of end of lines characters, that were added with the command gsub. The goal was to replace a pattern represented by a regular expression into a line breaker. 

```{r}
#gsub(pattern = '(?<=")\\s+"(?=[0-9][^-])', replace = '\n"', ., perl = TRUE)
```

After identifying all line breakers for particular .txt files, those files were imported using read_table. As an example is the code for the file of Parts_07

```{r}
#   x <- readLines(path) %>%
#     gsub(pattern = '""', replace = '"\n"', .) 
#   
#   for (i in 2:length(x) ) {
#     df <- read.table(textConnection(x[i]), header=TRUE)
# }
```


When all files were successfully imported, it was necessary to tidy the dataframes following the basic rules for Tidy Data. Some problems found were the different time formats, more than one column per information and other strange formats.

The `tidydate`function is called for all the files storing the data in two separate files. After reappending the readable date to the data frame, any row outside the date specicified 2015-2016 is disregarded. The helper function is utilized by some of both .csv and .txt import functions.

```{r}
tidyDate <- function(df) {
  print("tidyDate called!")
  
  daycount <- df$Produktionsdatum_Origin_01011970
  
  if (length(unique(df$origin))==1) {
    # Reformat date from data frame to fit as.Date
    betterDates <- as.Date(daycount, origin = "1970-01-01")
  } else {
    betterDates <- as.Date(daycount, origin = "1970-01-01")
    # Warn if multiple values are found, yet assume this was not intentional
    print("WARNING! Multiple values found!")
  }
  
  # Add date column with correctly formatted dates
  df$prod_date <- betterDates
  
  # Deleting rows that shall be disregarded because of date range
  df <- subset(df, !prod_date<"2015-01-01")
  df <- subset(df, !prod_date>"2016-12-31")

  return(df)
}
```

The `tidydate`function is used for uniting columns in the data frame, as some files show have the actual data written in different columns after a certain point. At the same time, the specified time range is selected and columns are renamed.

```{r}
tidyLong <- function(df) {
  print("tidyLong called!")
  
  # Unite related columns, since after some row number, values appear in different columns
  df <- unite(df, "prod_date", contains("Produktionsdatum"), sep="_")
  df <- unite(df, "global_id", contains("ID_T"), sep="_") 
  
  # Clean newly united col names from NA
  df$prod_date <- gsub(pattern="_NA|NA_", replace="", x=df$prod_date)
  df$global_id <- gsub(pattern="_NA|NA_", replace="", x=df$global_id)
  
  # Deleting rows that shall be disregarded because of date range
  df <- subset(df, !prod_date<"2015-01-01")
  df <- subset(df, !prod_date>"2016-12-31")
  
  return(df)
}
```

The `dropAndRename` function is utilized by some .txt import functions to unnecessary columns and rename the id. Furthermore, an import analysis is conducted, to scan the data frame for missing values (NA)

```{r}
dropAndRename <- function(df) {
  
  #  Drop columns (prod_date was appended as 11th column)
  df <- df[c(2, 10)]
  
  # Renaming cols
  names(df)[1] <- "global_id"
  
  # Check for NA values
  importAnalysis(df)
  
  return(df)
}
```

The following `importTXT` function assigns each plain text file based on their file name to a tidy function: 

```{r}
# Function to assign the correct import and tidy function to a .txt file
importTXT <- function(path) {
  print(path)
  
  if (str_detect(path, "T01.txt")) {
    print("tidyTXT_1 called")
    return(tidyTXT_1(path))
    
  } else if(str_detect(path, "T02.txt")) {
    print("tidyTXT_2 called")
    return(tidyTXT_2(path))
    
  } else if(str_detect(path, "T03.txt")) {
    print("tidyTXT_3 called")
    return(tidyTXT_3(path))
    
  } else if(str_detect(path, "T07.txt")) {
    print("tidyTXT_7 called")
    return(tidyTXT_7(path))
    
  } else if(str_detect(path, "T09.txt")) {
    print("tidyTXT_9 called")
    return(tidyTXT_9(path))
    
  } else if(str_detect(path, "T11.txt")) {
    print("tidyTXT_11 called")
    return(tidyTXT_11(path))
    
  } else if(str_detect(path, "T16.txt")) {
    print("tidyTXT_16 called")
    return(tidyTXT_16(path))
    
  } else if(str_detect(path, "T20.txt")) {
    print("tidyTXT_20 called")
    return(tidyTXT_20(path))
    
  } else if(str_detect(path, "T22.txt")) {
    print("tidyTXT_22 called")
    return(tidyTXT_22(path))
    
  } else if(str_detect(path, "T24.txt")) {
    print("tidyTXT_24 called")
    return(tidyTXT_24(path))
    
  } else if(str_detect(path, "T27.txt")) {
    print("tidyTXT_27 called")
    return(tidyTXT_27(path))
    
  } else if(str_detect(path, "T31.txt")) {
    print("tidyTXT_31 called")
    return(tidyTXT_31(path))
    
  } else if(str_detect(path, "T34.txt")) {
    print("tidyTXT_34 called")
    return(tidyTXT_34(path))
    
  } else if(str_detect(path, "T35.txt")) {
    print("tidyTXT_35 called")
    return(tidyTXT_35(path))
    
  } else if(str_detect(path, "T36.txt")) {
    print("tidyTXT_36 called")
    return(tidyTXT_36(path))
    
  } else if(str_detect(path, "T39.txt")) {
    print("tidyTXT_39 called")
    return(tidyTXT_39(path))
  }  
}
```



For the main analysis of the group only the ID columns and the Production Dates are necessary. In that way all the other columns were discharged. It is possible to verify how the functions were used for an example.

```{r}
tidyTXT_7 <- function(path){
  x <- readLines(path) %>%
    gsub(pattern = '""', replace = '"\n"', .) 
  
  for (i in 2:length(x) ) {
    df <- read.table(textConnection(x[i]), header=TRUE)
  }
  
  df <- tidyDate(df)
  
  # Drop columns and rename 
  df <- dropAndRename(df)
  
  return(df)
}

```


#### Components

Before attempting to import the files under the "Komponente" directory, every file needs to be checked for separators and patterns to work out the specifics of the import function and the required options. Some have working production date values, some contain an "origin" column and therefore include the date in POSIXct format. Distinctions must also be made for .txt files. Files with same patterns are put into the same list for future reference.

```{r component lists}
# Bestandteile files
# # CSV with pattern A (clean, 5 main cols)
BE_list_A <- c("Bestandteile_Komponente_K1BE1", "Bestandteile_Komponente_K1BE2", "Bestandteile_Komponente_K1DI1", 
               "Bestandteile_Komponente_K1DI2", "Bestandteile_Komponente_K6")

# #CSV with pattern B (clean, but only 4 main cols)
BE_list_B <- c("Bestandteile_Komponente_K2LE1", "Bestandteile_Komponente_K2LE2", "Bestandteile_Komponente_K2ST2", 
               "Bestandteile_Komponente_K3AG1", "Bestandteile_Komponente_K3AG2", "Bestandteile_Komponente_K3SG1", 
               "Bestandteile_Komponente_K3SG2", "Bestandteile_Komponente_K4", "Bestandteile_Komponente_K5")

# CSV with pattern B2 (clean, 4 main cols + extra cols)
BE_list_B2 <- c("Bestandteile_Komponente_K2ST1")

# #CSV with pattern C (clean, 6 main cols)
BE_list_C <- c("Bestandteile_Komponente_K7")

#####################################################

# Produktionsdatum files
# CSV with pattern A (clean, sep = ; 10 total cols, needs date cleaning)
Kcsv_list_A <- c("Komponente_K1BE2", "Komponente_K2ST2", "Komponente_K6")

# CSV with pattern B (fairly clean, sep = , 10 total cols, needs date cleaning)
Kcsv_list_B <- c("Komponente_K1BE1", "Komponente_K3SG2")

# CSV with pattern Cxyz1 (very dirty, sep = , 23 total cols with .x .y normal, ascending second col X1_1 skipping numbers,
# cols have different arrangement: x1,x1_1,ID_Motor.x,Proddatum.x,Herstllnr.x,Werknr.x,Fehlr.x,Fehlrdatum.x,FehlrFahr.x...
# from first col x1 = 1 values are in the first cols ID_Motor.x...
# from first col x1 = 477053 values are in the mid cols ID_Motor.y... 
# from first col x1 = 715579 values are in the Last cols ID_Motor...)
Kcsv_list_Cxyz1 <- c("Komponente_K1DI1")

# CSV with pattern Cxyz2 (very dirty, sep = , 23 total cols with .x .y normal, ascending second col X1_1 skipping numbers,
# cols have different arrangement: x1,x1_1,ID_Schaltung.x,Proddatum.x,Herstllnr.x,Werknr.x,Fehlr.x,Fehlrdatum.x,FehlrFahr.x...
# from first col x1 = 1 values are in the first cols ID_Schaltung.x...
# from first col x1 = 143116 values are in the mid cols ID_Schaltung.y... 
# from first col x1 = 381642 values are in the Last cols ID_Schaltung...)
Kcsv_list_Cxyz2 <- c("Komponente_K3AG1")

# CSV with pattern Cxy1 (very dirty, sep = , 16 total cols with .x .y, ascending second col X1_1 skipping numbers,
# cols have different arrangement: x1,x1_1,ID_Schaltung.x,Proddatum.x,Herstllnr.x,Werknr.x,Fehlr.x,Fehlrdatum.x,FehlrFahr.x...
# from first col x1 = 1 values are in the first cols ID_Schaltung.x...
# from first col x1 = 763284 values are in the mid cols ID_Schaltung.y...)
Kcsv_list_Cxy1 <- c("Komponente_K3SG1")

# CSV with pattern Cxy2 (very dirty, sep = , 16 total cols with .x .y, ascending second col X1_1 skipping numbers,
# cols have different arrangement: x1,x1_1,ID_Karosserie.x,Proddatum.x,Herstllnr.x,Werknr.x,Fehlr.x,Fehlrdatum.x,FehlrFahr.x...
# from first col x1 = 1 values are in the first cols ID_Karosserie.x...
# from first col x1 = 326477 values are in the mid cols ID_Karosserie.y...)
Kcsv_list_Cxy2 <- c("Komponente_K5")

# CSV with pattern Cxy3 (very dirty, sep = ; 16 total cols with .x .y, ascending second col X1_1 skipping numbers,
# cols have different arrangement: x1,x1_1,ID_Karosserie.x,Proddatum.x,Herstllnr.x,Werknr.x,Fehlr.x,Fehlrdatum.x,FehlrFahr.x...
# from first col x1 = 1 values are in the first cols ID_Karosserie.x...
# from first col x1 = 790866 values are in the mid cols ID_Karosserie.y...)
Kcsv_list_Cxy3 <- c("Komponente_K4")

######################################################

# TXT with pattern A (simple formatted txt sep = \t)
Ktxt_list_A <- c("K7.txt")

# TXT with pattern B (sep = "\")
Ktxt_list_B <- c("K2LE2.txt", "K3AG2.txt")

# TXT with pattern C (sep = "|"), date already clean
Ktxt_list_C <- c("K2ST1.txt")

# TXT with pattern D (sep = "\" zeilenende \t) SEE VICTORS CODE
Ktxt_list_D <- c("K1DI2.txt")

# TXT with pattern E (sep = "II", zeilenende "") SEE VICTORS CODE
Ktxt_list_E <- c("K2LE1.txt")


```

Next, the file paths need to be specified. For automation purposes, all paths are put in a list for looping. To differentiate between "Bestandteile" and data with production dates, the file path list with 32 entries is split into two vectors for each, with the former 16 files belonging to "Bestandteile".

```{r component paths}
# Get a char vector with all the paths
compFileNames <- list.files("Data/Komponente")
fullPath <- paste("Data/Komponente/", compFileNames, sep="")
BEVector <- fullPath[1:16]
compPathVector <- fullPath[17:32]
```

Now a function is needed to differentiate between the input file for import and select the appropriate function for that specific pattern. In order to follow the process, logging messages are created for each step of the import process. The following function systematically checks if the input path belongs to any of the predefined lists and selects the appropriate function when a Match is found. The order of patterns checked is intentionally set according to the sequence of entries within the patterns list to ensure no files are incorrectly matched to wrong import functions. The selected functions for import are yet to be defined. Note that some patterns are similar and the same import function can be applied for efficiency. For files with different separators, an option for separators is used.

```{r component function selecter}
# Function determines, based on predefined lists, which tidy function shall be applied
determineTidyFunction <- function(filePath) {
  
  if (length(which(str_detect(filePath, BE_list_A))) == 1){
    print(paste("found match in BE_list_A:", filePath)) # console logging
    tidyCSV_BE(filePath)
  } else if (length(which(str_detect(filePath, BE_list_B))) == 1){
    print(paste("found match in BE_list_B:", filePath)) # console logging
    tidyCSV_BE(filePath)
  } else if (length(which(str_detect(filePath, BE_list_B2))) == 1){
    print(paste("found match in BE_list_B2:", filePath)) # console logging
    tidyCSV_BE(filePath)
  } else if (length(which(str_detect(filePath, BE_list_C))) == 1){
    print(paste("found match in BE_list_C:", filePath)) # console logging
    tidyCSV_BE(filePath)
    
  } else if (length(which(str_detect(filePath, Kcsv_list_A))) == 1){
    print(paste("found match in Kcsv_list_A:", filePath)) # console logging
    tidyCSV_Kcsv_AB(filePath)
  } else if (length(which(str_detect(filePath, Kcsv_list_B))) == 1){
    print(paste("found match in Kcsv_list_B:", filePath)) # console logging
    tidyCSV_Kcsv_AB(filePath,",")
  } else if (length(which(str_detect(filePath, Kcsv_list_Cxyz1))) == 1){
    print(paste("found match in Kcsv_list_Cxyz1:", filePath)) # console logging
    tidyCSV_Kcsv_Cxyz(filePath,",")
  } else if (length(which(str_detect(filePath, Kcsv_list_Cxyz2))) == 1){
    print(paste("found match in Kcsv_list_Cxyz2:", filePath)) # console logging
    tidyCSV_Kcsv_Cxyz(filePath,",")
  } else if (length(which(str_detect(filePath, Kcsv_list_Cxy1))) == 1){
    print(paste("found match in Kcsv_list_Cxy1:", filePath)) # console logging
    tidyCSV_Kcsv_Cxyz(filePath,",")
  } else if (length(which(str_detect(filePath, Kcsv_list_Cxy2))) == 1){
    print(paste("found match in Kcsv_list_Cxy2:", filePath)) # console logging
    tidyCSV_Kcsv_Cxyz(filePath,",")
  } else if (length(which(str_detect(filePath, Kcsv_list_Cxy3))) == 1){
    print(paste("found match in Kcsv_list_Cxy3:", filePath)) # console logging
    tidyCSV_Kcsv_Cxyz(filePath)
    
  } else if (length(which(str_detect(filePath, Ktxt_list_A))) == 1){
    print(paste("found match in Ktxt_list_A:", filePath)) # console logging
    tidyTXT_A(filePath)
  } else if (length(which(str_detect(filePath, Ktxt_list_B))) == 1){
    print(paste("found match in Ktxt_list_B:", filePath)) # console logging
    tidyTXT_B(filePath)
  } else if (length(which(str_detect(filePath, Ktxt_list_C))) == 1){
    print(paste("found match in Ktxt_list_C:", filePath)) # console logging
    tidyTXT_C(filePath)
  } else if (length(which(str_detect(filePath, Ktxt_list_D))) == 1){
    print(paste("found match in Ktxt_list_D:", filePath)) # console logging
    tidyTXT_D(filePath)
  } else if (length(which(str_detect(filePath, Ktxt_list_E))) == 1){
    print(paste("found match in Ktxt_list_E:", filePath)) # console logging
    tidyTXT_E(filePath)
  } else {
    print("No Files in List")
  }
  
}
```

In addition, the global data frame variable "df" is defined to prepare for import.

```{r define df}
# define data frame variable, necessary for returning data frame from function
df <- 1
```

Now the import function for "Bestandteile" files is defined. Logging messages are applied to follow the progress. Since ";" is used as separator, as is common practice in Germany, read.csv2() can be used to import all "Bestandteile" files. The option "stringsAsFactors = FALSE" is necessary as R is likely to interpret string values as factors and result in errors during import when the first values in a column remain identical. Functions for tidying are also used immediately after import, as different patterns require different approaches to both import and tidying. The gather() function takes all attributes related to a part ID and joins them into one single attribute. Irrelevant colums such as the first two X and X1 are removed. A standardized column name is assigned to the component column. Every step as well as the result is assigned to the global variable df.

```{r tidyCSV_BE}
# Function to tidy CSV with format "BE"-------------------------------------------
# returns data frame


tidyCSV_BE <- function(path) {
  print(paste0("tidyCSV_BE called with path: ", path))
  
  # Read CSV and store in temporary data frame (df)
  df <<- read.csv2(path, stringsAsFactors = FALSE)
  
  # Gather all ID_T* in one column
  df <<- gather(df, part, part_global_id, -contains("X"), -contains("K"))
  
  # Delete redundant X1
  df$X1 <<- NULL
  df$X <<- NULL
  df$part <<- NULL
  names(df)[1] <<- "comp_global_id"
  
  # print(df)
  return(df)
}

```

Next, import functions for the remaining files with production date entries are defined. As files from both "Kcsv_list_A" and "Kcsv_list_B" can be imported and tidied with one function, an if else statement can help to select the appropriate import function read.csv() or read.csv2() within this function. After the initial import, the production date is converted from POSIXct format to a regular date format, the result is saved in the standardized column "prod_date". Afterwards, the irrelevant X1 column is removed, other unwanted columns are dropped and the remaining columns remaned to standardized names. Lastly, the results are filtered for the required period 2015-2016 and the remaining redundant columns such as "factory" and "oem" which are included in the unique "global_id" are dropped.

```{r tidyCSV_Kcsv_AB}
# Function to tidy CSV with format "Kcsv"----------------------------------------
# returns data frame
tidyCSV_Kcsv_AB <- function(path, delim = ";") {
  print(paste0("tidyCSV_AB called with path: ", path))
  
  #Read CSV and store in temporary data frame (df)
  if (delim == ",") {
    df <<- read.csv(path, stringsAsFactors = FALSE)
  } else {
    df <<- read.csv2(path, stringsAsFactors = FALSE)
  }
  
  #Store all counted days in vector
  daycount <<-df$Produktionsdatum_Origin_01011970
  
  #Check if origin has a single unique value and reformat
  if (length(unique(df$origin))==1) {
    # Reformat date from data frame to fit as.Date
    betterDates <<- as.Date(daycount, origin = "1970-01-01")
  } else {
    print("Aborting, multiple values found for 'origin'")
  }
  
  # Add date column with correctly formatted dates
  df$prod_date <<- betterDates
  
  # Tidy: Deleting Columns
  #Check if X == X1
  if (sum(!df$X == df$X1) == 0) {1
    # Delete redundant X1
    df$X1 <<- NULL
    print("Redundant Column X1 deleted")
  }
  
  # Drop columns
  df <<- df[-c(5:9)]
  
  # Renaming cols
  names(df)[1] <<- "id"
  names(df)[2] <<- "global_id"
  names(df)[3] <<- "oem"
  names(df)[4] <<- "factory"
  
  # filter for 2015-2016 and only ID and prod_date cols
  dateFilter()
  reformatCols()
  
  # print(df)
  return(df)
}
```

The import function for multiple patterns with column name suffixes ".x" and ".y" is shown below. Apart from the previously described steps, unite() is used to unite the required values located in different columns into one column. Since NA entries were included thorugh this method, gsub() takes care of their removal. Furthermore, as the resulting prod_date column has been edited with unite() and gsub(), its class is no longer date, but character. A simple as.Date() function solves this issue to prevent errors in future transformations.

```{r tidyCSV_Kcsv_Cxyz}
tidyCSV_Kcsv_Cxyz <- function(path, delim = ";") {
  print(paste0("tidyCSV_Kcsv_Cxyz called with path: ", path))
  
  #Read CSV and store in temporary data frame (df)
  if (delim == ",") {
    df <<- read.csv(path, stringsAsFactors = FALSE)
  } else {
    df <<- read.csv2(path, stringsAsFactors = FALSE)
  }
  
  # Combine .x .y .z cols into one
  df <<- unite(df, "prod_date", contains("Produktionsdatum"), sep="_")
  df <<- unite(df, "oem", contains("Herstellernummer"), sep="_")
  df <<- unite(df, "factory", contains("Werksnummer"), sep="_")
  df <<- unite(df, "global_id", contains("ID"), sep="_")
  
  # Clean newly united col names from NA
  df$prod_date <<- gsub(pattern="_NA|NA_",replace="",x=df$prod_date)
  df$oem <<- gsub(pattern="_NA|NA_",replace="",x=df$oem)
  df$factory <<- gsub(pattern="_NA|NA_",replace="",x=df$factory)
  df$global_id <<- gsub(pattern="_NA|NA_",replace="",x=df$global_id)
  names(df)[1] <<- "id"
  
  # Delete unnecessary cols, reorder
  df <<- subset(df, select=c(1,3,5,6,4)) 
  
  # filter for 2015-2016
  dateFilter()
  
  # Reformat date col from chr to date
  df$prod_date <<- as.Date(df$prod_date)
  
  # only ID and prod_date cols
  reformatCols()
  
  # print(df)
  return(df)
}
```

As for the .txt files, the following functions use read.table() to import them. Here, the separator is the main difference. The rest of the functions has been explained previously.

```{r tidyTXT_A & B & C}
tidyTXT_A <- function(path){
  print(paste0("tidyTXT_A called with path: ", path))
  
  #Read TXT and store in temporary data frame (df)
  df <<- read.table(path, stringsAsFactors = FALSE)
  
  #Store all counted days in vector
  daycount <<-df$Produktionsdatum_Origin_01011970
  
  #Check if origin has a single unique value and reformat
  if (length(unique(df$origin))==1) {
    # Reformat date from data frame to fit as.Date
    betterDates <<- as.Date(daycount, origin = "1970-01-01")
  } else {
    print("Aborting, multiple values found!")
  }
  
  # Add date column with correctly formatted dates
  df$prod_date <<- betterDates
  
  # Drop columns
  df <<- df[-c(5:9)]
  
  # Renaming cols
  names(df)[1] <<- "id"
  names(df)[2] <<- "global_id"
  names(df)[3] <<- "oem"
  names(df)[4] <<- "factory"
  
  # filter for 2015-2016 and only ID and prod_date cols
  dateFilter()
  reformatCols()
  
  # print(df)
  return(df)
}

tidyTXT_B <- function(path){
  print(paste0("tidyTXT_B called with path: ", path))
  
  #Read TXT and store in temporary data frame (df)
  df <<- read.table(path, sep = "\\",stringsAsFactors = FALSE)
  
  #Store all counted days in vector
  daycount <<-df$Produktionsdatum_Origin_01011970
  
  #Check if origin has a single unique value and reformat
  if (length(unique(df$origin))==1) {
    # Reformat date from data frame to fit as.Date
    betterDates <<- as.Date(daycount, origin = "1970-01-01")
  } else {
    print("Aborting, multiple values found!")
  }
  
  # Add date column with correctly formatted dates
  df$prod_date <<- betterDates
  
  # Drop columns
  df <<- df[-c(5:9)]
  
  # Renaming cols
  names(df)[1] <<- "id"
  names(df)[2] <<- "global_id"
  names(df)[3] <<- "oem"
  names(df)[4] <<- "factory"
  
  # filter for 2015-2016 and only ID and prod_date cols
  dateFilter()
  reformatCols()
  
  # print(df)
  return(df)
}

tidyTXT_C <- function(path){
  print(paste0("tidyTXT_C called with path: ", path))
  
  #Read TXT and store in temporary data frame (df)
  df <<- read.table(path, sep = "|", stringsAsFactors = FALSE)
  
  # Renaming cols
  names(df)[1] <<- "id"
  names(df)[2] <<- "global_id"
  names(df)[3] <<- "prod_date"
  names(df)[4] <<- "oem"
  names(df)[5] <<- "factory"
  
  # Delete unnecessary cols, reorder
  df <<- subset(df, select=c(1,2,4,5,3)) 
  
  # filter for 2015-2016
  dateFilter()
  
  # Reformat date col from chr to date
  df$prod_date <<- as.Date(df$prod_date)
  
  # only ID and prod_date cols
  reformatCols()
  
  # print(df)
  return(df)
}
```

Some files require more complicated preparations before import. For .txt files missing line endings, readLines() has to be applied and the result cleaned with gsub() before conventional read.table() functions work. Additional steps as explained previously for tidying might also be applied when required.

```{r tidyTXT_D & E}
tidyTXT_D <- function(path){
  print(paste0("tidyTXT_D called with path: ", path))
  
  # import .txt file by lines and add line endings and remove interfering chars
  readLines(path) %>% 
    gsub(pattern='"\t"', replace = '"\n"') %>% 
    gsub(pattern='\t', replace = "") %>% 
    paste0('""\\',.) %>% 
    read.table(text=., sep = "\\", header=TRUE, stringsAsFactors = FALSE) ->> df
  
  #Store all counted days in vector
  daycount <<-df$Produktionsdatum_Origin_01011970
  
  #Check if origin has a single unique value and reformat
  if (length(unique(df$origin))==1) {
    # Reformat date from data frame to fit as.Date
    betterDates <<- as.Date(daycount, origin = "1970-01-01")
  } else {
    print("Aborting, multiple values found!")
  }
  
  # Add date column with correctly formatted dates
  df$prod_date <<- betterDates
  
  
  # Tidy: Deleting Columns
  #Check if X1 == X1_1
  if (sum(!df$X == df$X1) == 0) {1
    # Delete X1_1
    df$X1 <<- NULL
    print("Redundant Column X1 deleted")
  }
  
  # Drop columns
  df <<- df[-c(5:9)]
  
  # Renaming cols
  names(df)[1] <<- "id"
  names(df)[2] <<- "global_id"
  names(df)[3] <<- "oem"
  names(df)[4] <<- "factory"
  
  # filter for 2015-2016 and only ID and prod_date cols
  dateFilter()
  reformatCols()
  
  # print(df)
  return(df)
}

tidyTXT_E <- function(path){
  print(paste0("tidyTXT_E called with path: ", path))
  
  # Unfinished Code, requires more testing
  readLines(path) %>% 
    gsub(pattern='', replace = '\n') %>% 
    gsub(pattern = "II", replace = ";") %>% 
    paste0('"";',.) %>% 
    read.table(text=., sep = ";", header=TRUE,stringsAsFactors = FALSE) ->> df
  
  # combine .x .y into one
  df <<- unite(df, "prod_date", contains("Produktionsdatum"), sep="_")
  df <<- unite(df, "oem", contains("Herstellernummer"), sep="_")
  df <<- unite(df, "factory", contains("Werksnummer"), sep="_")
  df <<- unite(df, "global_id", contains("ID"), sep="_")
  
  # Clean newly united col names from NA
  df$prod_date <<- gsub(pattern="_NA|NA_",replace="",x=df$prod_date)
  df$oem <<- gsub(pattern="_NA|NA_",replace="",x=df$oem)
  df$factory <<- gsub(pattern="_NA|NA_",replace="",x=df$factory)
  df$global_id <<- gsub(pattern="_NA|NA_",replace="",x=df$global_id)
  names(df)[1] <<- "id"
  
  # Delete unncessary cols, reorder
  df <<- subset(df, select=c(1,3,5,6,4))
  
  # filter for 2015-2016
  dateFilter()
  
  # Reformat date col from chr to date
  df$prod_date <<- as.Date(df$prod_date)
  
  # only ID and prod_date cols
  reformatCols()
  
  # print(df)
  return(df)
}
```

Some previously used functions such as dateFilter(), reformatCols() are then defined. Also na_checker() will be used later on to guarantee there are no NA entries present.

```{r Support Functions}
#################################
# Support Functions
#################################

# Deleting rows that shall be disregarded because of date range
dateFilter <- function() {
  
  df <<- subset(df, !prod_date<"2015-01-01")
  df <<- subset(df, !prod_date>"2016-12-31")
}

# Remove ID vol, row names, rename cols to comp_*
reformatCols <- function() {
  
  df <<- subset(df, select=c(2,5))
  rownames(df) <<- c()
  names(df) <<- c("comp_global_id", "comp_prod_date")
}

# Check for NA
na_checker <- function() {
  print(paste0("comp_df has ", sum(is.na(comp_df))," NAs"))
  print(paste0("BE_comp_df has ", sum(is.na(BE_comp_df))," NAs"))
}

```

Now, a function for the execution of all the component import code is defined. By looping through the path vectors, BE_list and comp_df_list are filled with imported data frames as list entries.

```{r startImportComp}
#################################
# Run the function / Script
#################################

# Call this function to start importing all data from ./Komponenten/
startImportComp <- function() {
  print("starting importing Bestandteile")
  BE_list <<- list()
  for (i in seq_along(BEVector)) {
    BE_list[[i]] <<- determineTidyFunction(BEVector[i])
  }
  print("starting importing Komponenten")
  comp_df_list <<- list()
  for (i in seq_along(compPathVector)) {
    comp_df_list[[i]] <<- determineTidyFunction(compPathVector[i])
    
  }
}
```

Once the lists containing data frames are created, the following functions are defined which will provide further tidying by checking for duplicate comp_global_id and part_global_id entries, as these need to be unique. As long as the output is "0", there are no duplicates.

```{r dup checkers}

# check for component id duplicates in comp_df_list
dup_checker_comp_df_list <- function() {
  for (i in 1:16) {
    print(paste0("Number of duplicates in comp_df_list ", i, "/16: ", sum(duplicated(comp_df_list[[i]]$comp_global_id))))
  }
}

# check for part id duplicates in BE_df_list
dup_checker_BE_list <- function() {
  for (i in 1:16) {
    print(paste0("Number of duplicates in comp_df_list ", i, "/16: ", sum(duplicated(BE_list[[i]]$part_global_id))))
  }
}
```

Lastly, two functions are defined which use bind_rows() to combine all data frames in the lists into two large data frames respectively. The work is progress can be followed with log messages while for loops apply bind_rows() onto the list entries, resulting in the two large data frames comp_df with the columns "comp_global_id" and "comp_prod_date" and BE_comp_df with the columns "comp_global_id" and "part_global_id". These two data frames are ready to be merged with each other and with the vehicle and part data frames.

```{r comp df links}
# combine comp_df_list entries into one data frame: comp_df
link_comp_df_list <- function() {
  print("Linking comp_df_list dfs into one")
  comp_df <<- comp_df_list[[1]]
  print(paste0("df added:","1/16"))
  for (i in 2:length(comp_df_list)) {
    comp_df <<- bind_rows(comp_df, comp_df_list[[i]])
    print(paste0("df added:",i,"/16"))
  }
  print("comp_df created successfully!")
}

# combine BE_list entries into one data frame: BE_comp_df
link_BE_list <- function() {
  print("Linking BE_list dfs into one")
  BE_comp_df <<- BE_list[[1]]
  print(paste0("df added:","1/16"))
  for (i in 2:16) {
    BE_comp_df <<- bind_rows(BE_comp_df, BE_list[[i]])
    print(paste0("df added:",i,"/16"))
  }
  print("BE_comp_df created successfully!")
}
```


#### Vehicles

add import scripts



### 3. Transform

Add information for the merged data

### 4. Shiny App & Visualization

Add Shiny, Boxplit info

### 5. Analysis

Insert analysis data and information
